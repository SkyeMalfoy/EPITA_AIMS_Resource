{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (3.5.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (4.63.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: setuptools in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.22.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.10.9)\n",
      "Requirement already satisfied: jinja2 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (2.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Users/skyemalfoy/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'serialized_options'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/__init__.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[1;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/errors.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/compat.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_array\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcPickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/thinc/__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[1;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregistry\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m ]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/thinc/config.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, Promise, VARIABLE_RE\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Decorator\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mregistry\u001b[39;00m(confection\u001b[38;5;241m.\u001b[39mregistry):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     optimizers: Decorator \u001b[38;5;241m=\u001b[39m catalogue\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthinc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, entry_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/thinc/types.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m has_cupy, cupy\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_cupy:\n\u001b[1;32m     11\u001b[0m     get_array_module \u001b[38;5;241m=\u001b[39m cupy\u001b[38;5;241m.\u001b[39mget_array_module\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/thinc/compat.py:54\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m     torch_version \u001b[38;5;241m=\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdlpack\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     has_tensorflow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/context.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coordination_config_pb2\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/core/framework/function_pb2.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attr_value_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m node_def_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_node__def__pb2\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m op_def_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_op__def__pb2\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/core/framework/attr_value_pb2.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__pb2\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/core/framework/tensor_pb2.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resource_handle_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/core/framework/resource_handle_pb2.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n\u001b[1;32m     20\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[1;32m     21\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow/core/framework/resource_handle.proto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m   package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m   ,\n\u001b[1;32m     27\u001b[0m   dependencies\u001b[38;5;241m=\u001b[39m[tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[0;32m---> 18\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m \u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFileDescriptor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m  \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtensorflow/core/framework/tensor_shape.proto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m  \u001b[49m\u001b[43mpackage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtensorflow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m  \u001b[49m\u001b[43msyntax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproto3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m  \u001b[49m\u001b[43mserialized_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_b\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\030\u001b[39;49;00m\u001b[38;5;124;43morg.tensorflow.frameworkB\u001b[39;49m\u001b[38;5;130;43;01m\\021\u001b[39;49;00m\u001b[38;5;124;43mTensorShapeProtosP\u001b[39;49m\u001b[38;5;130;43;01m\\001\u001b[39;49;00m\u001b[38;5;124;43mZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\u001b[39;49m\u001b[38;5;130;43;01m\\370\u001b[39;49;00m\u001b[38;5;130;43;01m\\001\u001b[39;49;00m\u001b[38;5;130;43;01m\\001\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m  \u001b[49m\u001b[43mserialized_pb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_b\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m,tensorflow/core/framework/tensor_shape.proto\u001b[39;49m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtensorflow\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43mz\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x10\u001b[39;49;00m\u001b[38;5;124;43mTensorShapeProto\u001b[39;49m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x03\u001b[39;49;00m\u001b[38;5;130;43;01m\\x64\u001b[39;49;00m\u001b[38;5;124;43mim\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\x02\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x03\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\x0b\u001b[39;49;00m\u001b[38;5;130;43;01m\\x32\u001b[39;49;00m\u001b[38;5;124;43m .tensorflow.TensorShapeProto.Dim\u001b[39;49m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;130;43;01m\\x14\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x0c\u001b[39;49;00m\u001b[38;5;124;43munknown_rank\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\x03\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\x08\u001b[39;49;00m\u001b[38;5;130;43;01m\\x1a\u001b[39;49;00m\u001b[38;5;124;43m!\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x03\u001b[39;49;00m\u001b[38;5;130;43;01m\\x44\u001b[39;49;00m\u001b[38;5;124;43mim\u001b[39;49m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;130;43;01m\\x0c\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x04\u001b[39;49;00m\u001b[38;5;124;43msize\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\x03\u001b[39;49;00m\u001b[38;5;130;43;01m\\x12\u001b[39;49;00m\u001b[38;5;130;43;01m\\x0c\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x04\u001b[39;49;00m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;130;43;01m\\x02\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43mB\u001b[39;49m\u001b[38;5;130;43;01m\\x87\u001b[39;49;00m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\x18\u001b[39;49;00m\u001b[38;5;124;43morg.tensorflow.frameworkB\u001b[39;49m\u001b[38;5;130;43;01m\\x11\u001b[39;49;00m\u001b[38;5;124;43mTensorShapeProtosP\u001b[39;49m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;124;43mZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\u001b[39;49m\u001b[38;5;130;43;01m\\xf8\u001b[39;49;00m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;130;43;01m\\x01\u001b[39;49;00m\u001b[38;5;130;43;01m\\x62\u001b[39;49;00m\u001b[38;5;130;43;01m\\x06\u001b[39;49;00m\u001b[38;5;124;43mproto3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m _TENSORSHAPEPROTO_DIM \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[1;32m     30\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDim\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow.TensorShapeProto.Dim\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m182\u001b[39m,\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     66\u001b[0m _TENSORSHAPEPROTO \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[1;32m     67\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensorShapeProto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     68\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow.TensorShapeProto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m182\u001b[39m,\n\u001b[1;32m    101\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'serialized_options'"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n65301OMyMnL"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmBBgnbCHV59"
   },
   "source": [
    "**SpaCy** is a popular library for natural language processing.<br>In the following exercise, you will use the `spacy.load()` function to tokenize the text.\n",
    "\n",
    "The function's syntax is `spacy.load(name, **kwargs)`.\n",
    "\n",
    "*   The **`name`** parameter is the name of the model to be loaded. This could be a package name, a shortcut link, or a path to the model data directory. SpaCy provides several pre-trained models that you can download and use in your code. These models are trained on a variety of data and support a wide range of languages.\n",
    "*   `**kwargs` represents keyword arguments which are optional.\n",
    "\n",
    "SpaCy offers several pre-trained models for the English language which are trained on different types and amounts of data:\n",
    "\n",
    "* **`en_core_web_sm`**: This is the smallest model and is great for getting started with. Assigns word vectors, Part-of-Speech tags, dependency parses and named entities.\n",
    "\n",
    "* **`en_core_web_md`**: This is the medium version of the English model.\n",
    "\n",
    "* **`en_core_web_lg`**: This is the large version of the English model.\n",
    "\n",
    "* **`en_core_web_trf`**: This model is a transformer-based pipeline for English text, using the base model of the transformer architecture. The pipeline includes a tokenizer, a transformer, a part-of-speech tagger, a parser, an attribute ruler, a lemmatizer, and a named entity recognizer.\n",
    "\n",
    "**1. Load the `en_core_web_sm` model in the `spacy.load()` function and use it on the following sentence.**\n",
    "\n",
    "*Tips: Store the spacy.load() function in a variable and then use that variable as a regular python function to transform the text.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJR-aqE3SCUP"
   },
   "outputs": [],
   "source": [
    "sentence = \"This is the 1st part of the 'NLP for chatbots' exercise sheet !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (3.5.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: setuptools in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: jinja2 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.10.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (4.63.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.22.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (2.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Users/skyemalfoy/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yMF7ur69RaQc"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/__init__.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[1;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/errors.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/compat.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_array\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcPickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/thinc/__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[1;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregistry\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m ]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/thinc/config.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, Promise, VARIABLE_RE\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Decorator\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mregistry\u001b[39;00m(confection\u001b[38;5;241m.\u001b[39mregistry):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     optimizers: Decorator \u001b[38;5;241m=\u001b[39m catalogue\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthinc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, entry_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/thinc/types.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m has_cupy, cupy\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_cupy:\n\u001b[1;32m     11\u001b[0m     get_array_module \u001b[38;5;241m=\u001b[39m cupy\u001b[38;5;241m.\u001b[39mget_array_module\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/thinc/compat.py:54\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m     torch_version \u001b[38;5;241m=\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdlpack\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     has_tensorflow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/__init__.py:37\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/context.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coordination_config_pb2\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/core/framework/function_pb2.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attr_value_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m node_def_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_node__def__pb2\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m op_def_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_op__def__pb2\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/core/framework/attr_value_pb2.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__pb2\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/core/framework/tensor_pb2.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resource_handle_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/core/framework/resource_handle_pb2.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n\u001b[1;32m     20\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[1;32m     21\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow/core/framework/resource_handle.proto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m   package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m   ,\n\u001b[1;32m     27\u001b[0m   dependencies\u001b[38;5;241m=\u001b[39m[tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:36\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m     18\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[1;32m     19\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow/core/framework/tensor_shape.proto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m   package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m   serialized_pb\u001b[38;5;241m=\u001b[39m_b(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m,tensorflow/core/framework/tensor_shape.proto\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mz\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x10\u001b[39;00m\u001b[38;5;124mTensorShapeProto\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x64\u001b[39;00m\u001b[38;5;124mim\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x02\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\x0b\u001b[39;00m\u001b[38;5;130;01m\\x32\u001b[39;00m\u001b[38;5;124m .tensorflow.TensorShapeProto.Dim\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x14\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x0c\u001b[39;00m\u001b[38;5;124munknown_rank\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\x08\u001b[39;00m\u001b[38;5;130;01m\\x1a\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x44\u001b[39;00m\u001b[38;5;124mim\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x0c\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124msize\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x0c\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124mname\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x02\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mB\u001b[39m\u001b[38;5;130;01m\\x87\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;124morg.tensorflow.frameworkB\u001b[39m\u001b[38;5;130;01m\\x11\u001b[39;00m\u001b[38;5;124mTensorShapeProtosP\u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124mZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\u001b[39m\u001b[38;5;130;01m\\xf8\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;130;01m\\x62\u001b[39;00m\u001b[38;5;130;01m\\x06\u001b[39;00m\u001b[38;5;124mproto3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     29\u001b[0m _TENSORSHAPEPROTO_DIM \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[1;32m     30\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDim\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow.TensorShapeProto.Dim\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m   filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m   file\u001b[38;5;241m=\u001b[39mDESCRIPTOR,\n\u001b[1;32m     34\u001b[0m   containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m   fields\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m---> 36\u001b[0m     \u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFieldDescriptor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtensorflow.TensorShapeProto.Dim.size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpp_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhas_default_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessage_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menum_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontaining_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m      \u001b[49m\u001b[43mis_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextension_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43mserialized_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDESCRIPTOR\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     43\u001b[0m     _descriptor\u001b[38;5;241m.\u001b[39mFieldDescriptor(\n\u001b[1;32m     44\u001b[0m       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow.TensorShapeProto.Dim.name\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     45\u001b[0m       number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, cpp_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     46\u001b[0m       has_default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, default_value\u001b[38;5;241m=\u001b[39m_b(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     47\u001b[0m       message_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, enum_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m       is_extension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, extension_scope\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m       serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, file\u001b[38;5;241m=\u001b[39mDESCRIPTOR),\n\u001b[1;32m     50\u001b[0m   ],\n\u001b[1;32m     51\u001b[0m   extensions\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     52\u001b[0m   ],\n\u001b[1;32m     53\u001b[0m   nested_types\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m     54\u001b[0m   enum_types\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     55\u001b[0m   ],\n\u001b[1;32m     56\u001b[0m   serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     57\u001b[0m   is_extendable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     58\u001b[0m   syntax\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproto3\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     59\u001b[0m   extension_ranges\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m     60\u001b[0m   oneofs\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     61\u001b[0m   ],\n\u001b[1;32m     62\u001b[0m   serialized_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m149\u001b[39m,\n\u001b[1;32m     63\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m182\u001b[39m,\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     66\u001b[0m _TENSORSHAPEPROTO \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[1;32m     67\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensorShapeProto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     68\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow.TensorShapeProto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m182\u001b[39m,\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    103\u001b[0m _TENSORSHAPEPROTO_DIM\u001b[38;5;241m.\u001b[39mcontaining_type \u001b[38;5;241m=\u001b[39m _TENSORSHAPEPROTO\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/google/protobuf/descriptor.py:561\u001b[0m, in \u001b[0;36mFieldDescriptor.__new__\u001b[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, name, full_name, index, number, \u001b[38;5;28mtype\u001b[39m, cpp_type, label,\n\u001b[1;32m    556\u001b[0m             default_value, message_type, enum_type, containing_type,\n\u001b[1;32m    557\u001b[0m             is_extension, extension_scope, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m             serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    559\u001b[0m             has_default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, containing_oneof\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    560\u001b[0m             file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m   \u001b[43m_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_CheckCalledFromGeneratedFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m is_extension:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _message\u001b[38;5;241m.\u001b[39mdefault_pool\u001b[38;5;241m.\u001b[39mFindExtensionByName(full_name)\n",
      "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEbIk22OKtkP"
   },
   "source": [
    "**2. What is the type of the result ?**\n",
    "\n",
    "*Tips: Use `type()` on the output of question 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5fi-w9tRatC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70XRUE0OLIfS"
   },
   "source": [
    "The function returns a **Doc object**, which is typically named nlp in SpaCy code.<br> This Doc object contains the processing pipeline and language-specific rules for tokenization.\n",
    "\n",
    "In our case, the Doc object is essentially a sequence of **Token objects**. The Token class in SpaCy represents a single token  that might be a word, punctuation symbol, whitespace, etc. Each Token object has a number of attributes that provide information about the token.\n",
    "\n",
    "The **`.text`** attribute of the Token object returns the original text of the token while the .is_alpha, .is_punct and .is_stop respectively returns True if the token is a number or letter, is a punctuation or is a stop word.\n",
    "\n",
    "**3. Iterate over the Doc object obtained at question 1 and print the `text`, `is_alpha`, `is_punct` and `is_stop`  attributes of each token.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZPbERnkRbL-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzmotiuSSZQK"
   },
   "source": [
    "SpaCy tokenization is powerful and can handle various specific cases.<br>\n",
    "For example, it can be tricky to correctly tokenize the sentence \"Dr. Smith graduated from Oxford University in 2023.\" because of the \"Dr.\" abbreviation, but SpaCy manages it.\n",
    "\n",
    "**4. Tokenize the aforementioned sentence to see what happens to the \"Dr.\" abbreviation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjqZ9hO8Rb6X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-UNd0iOTZaM"
   },
   "source": [
    "Here is a bunch of other particular cases:\n",
    "\n",
    "*   \"New York City is often just called NYC.\"\n",
    "*   \"I paid $50 for this T-shirt.\"\n",
    "*   \"She lives in St. Petersburg, Russia.\"\n",
    "*   \"We're visiting our cousins in San Francisco.\"\n",
    "*   \"He said, 'The party starts at 8 p.m.'\"\n",
    "*   \"I visited the website www.example.com.\"\n",
    "*   \"It's 100F outside.\"\n",
    "*   \"She got an A+ in her English test.\"\n",
    "*   \"The email address is example@example.com.\"\n",
    "\n",
    "**5. Tokenize the aforementionned cases and find their distinctive characteristic.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Xq-uvX8RdBr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2OFypZWyTjZ"
   },
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAG9IboDcZtw"
   },
   "source": [
    "SpaCy doesn't include a stemmer but fortunately, **NLTK**, the Natural Language Toolkit, does include a stemming option.\n",
    "\n",
    "NLTK has two different main stemmers :\n",
    "\n",
    "* The **Porter Stemmer** is one of the most popular stemming algorithms. It's a rule-based algorithm that strips suffixes based on a series of predefined rules. These rules define whether or not a word can have a certain suffix removed, often by considering the length of the word or what characters it ends with.\n",
    "\n",
    "* The **Snowball stemmer**, is a slightly improved version of the original Porter stemmer both in terms of logic and speed.\n",
    "\n",
    "The Porter Stemmer algorithm can be found on the NLTK libray [here](https://www.nltk.org/api/nltk.stem.porter.html) and can be imported from `nltk.stem`.\n",
    "\n",
    "**1. Use the Porter Stemmer algorithm to stem the following sentences.**\n",
    "\n",
    "*Tips: the `PorterStemmer() `class from nltk can stem the words individually with its `stem(word, to_lowercase=True)` method.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dywfeBdfElg"
   },
   "outputs": [],
   "source": [
    "sentences = [\"The runner was running in a running event. He outran all the other competitors.\",\n",
    "             \"I love studying different studies related to environmental sustainability.\",\n",
    "             \"The worker's determination determined the successful outcome of the project.\",\n",
    "             \"She's investing in investments that will likely appreciate significantly.\",\n",
    "             \"The geese are flying in a loose formation in the sky.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mvmEdy7ReE_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRJItAs6gcMg"
   },
   "source": [
    "**Lemmatization** is generally more sophisticated than stemming because it takes into account the morphological analysis of the words and always returns a valid word that exists in the language.\n",
    "\n",
    "It can easily be done using the SpaCy library. **`spacy.load()`** creates a Doc object object that contains token objects. Each token in the Doc object includes a variety of attribute, one of which is **`token.lemma_`** that provides the lemmatized form of the word.\n",
    "\n",
    "**2. Use the `lemma_` attribute to extract the lemmatized form of the words of the aforementionned sentences.<br>Is it better ? Why ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpLhaPdNRemT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28_M2H4Xys2U"
   },
   "source": [
    "# Part of Speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OREbsNMxivlL"
   },
   "source": [
    "**Part of Speech tagging** (POS tagging) can easily be done with SpaCy. Similarly to the lemmatization, POS tagging is performed automatically when you create a Doc object. Each token in the Doc object includes two attributes for POS tagging:\n",
    "\n",
    "*   the **pos_** attribute provides a coarse-grained part-of-speech tag.\n",
    "*   the **tag_** attribute provides provides a fine-grained part-of-speech tag. It includes more specific information about the word's morphological features, and the specific set of tags used can vary between languages.\n",
    "\n",
    "At first, most tags are complex and may be hard to understand. To alleviate this issue, SpaCy propose a **`spacy.explain()`** function that explains the various linguistic annotations that it can produce by returning a brief description of what the term means.\n",
    "\n",
    "**1. Use the `pos` and the `tag` attribute to get POS tagging of these two sentences.**\n",
    "\n",
    "*Tips: Don't forget to use the `spacy.explain()` function around the POS tag.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X9NlyHxAl_cE"
   },
   "outputs": [],
   "source": [
    "sentence_1 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "sentence_2 = \"Eating apples and oranges daily improves your health.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XxUE07gRfxP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btp5sdC8pVif"
   },
   "source": [
    "**displaCy** is SpaCy's built-in library for visualizing dependency parse trees and named entities in a sentence. It is a great tool for exploring and understanding the output of SpaCy's linguistic annotation models.\n",
    "\n",
    "> *A **dependency parse tree** is a way of showing how words in a sentence relate to each other. It's called a \"tree\" because it starts with a root word (usually the main verb), and every other word branches off from it or from other words. The \"branches\" show which words depend on which other words to make sense.*\n",
    "\n",
    "displaCy has two main function to visualize the sentences:\n",
    "\n",
    "\n",
    "*   **`displaCy.serve(doc, style)`** that starts a web server to display the visualization, and is not intended for use in production environments.\n",
    "*   **`displaCy.render(doc, style)`** that generates the tree as an image. This function should be use in Production as well as in Jupyter Notebooks.\n",
    "\n",
    "**2. Use the displaCy library to render the dependency parse trees of the two aforementionned sentences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2PXuxPQRgdG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5lKJE-9iNog"
   },
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vySDGR1ysmWE"
   },
   "source": [
    "SpaCy has a **named entity recognition model** that supports 18 entity types, such as PERSON, NORP (nationalities, religious and political groups), FAC (buildings, airports etc.), ORG (organizations), GPE (countries, cities, states), and so on.\n",
    "\n",
    "The spaCy Doc object object has an **`.ents`** property that can be used to extract named entities. You can then access the entity text, starting position, ending position and name with the **`.text`**, **`.start_char`**, **`.end_char`** and the **`.label`** attributes respectively.\n",
    "\n",
    "**1. Find the different named entities of each of the following sentences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t068wijctyMv"
   },
   "outputs": [],
   "source": [
    "sentence_1 = \"Microsoft, headquartered in Redmond, Washington, acquired LinkedIn for $26.2 billion in 2016, which was their biggest deal to date.\"\n",
    "sentence_2 = \"Emma Watson, a British actress best known for her role in the Harry Potter series, graduated from Brown University in Providence, Rhode Island in 2014.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nC_Rwl6FRhdm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMM8a5RsunRY"
   },
   "source": [
    "**2. Use displaCy to display the two sentences with their respective named entities.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4cgflgdRiCM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akSfTimKyNjb"
   },
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5FVqen43iKt"
   },
   "outputs": [],
   "source": [
    "# the en_core_web_sm model doesn't have any vector\n",
    "# thus, word embedding can only be done with the en_core_web_md or better\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EO77R2x494oC"
   },
   "source": [
    "In SpaCy, both Doc and Token objects have a **`.vector`** attribute.<br> This is a n-dimensional representation of the word or document using real numbers: the **word embedding**.\n",
    "\n",
    "In SpaCy, the sentence embedding is an average of its word embeddings.<br>Averaging word embeddings to get a sentence embedding is a simple and fast method, but it can lose important information.<br> Better methods like RNNs and Transformers have been developed to take into account the order of words and the context they are used in.\n",
    "\n",
    "**1. Display the word embeddings of the following word and sentence using the `.vector` attribute.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4g6VaH2944a"
   },
   "outputs": [],
   "source": [
    "word = \"red\"\n",
    "sentence = \"Jasmine felt joy noticing the change in her garden. Once pale, the roses were now a vibrant fuchsia, their color like a sunset. Her marigolds had proliferated, their numbers mirroring stars in the night sky. Amidst this spectacle, her delight soared like the blooming flowers.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozR_cEiaRjVq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcZT7GN8Afvl"
   },
   "source": [
    "**2. Reduce the dimensionality of each word of the aforementionned sentence to a 2D representation using t-SNE from sklearn.<br> Display a 2D graph with the words.<br>Can you find related words ?**\n",
    "\n",
    "*Tips: The result will be cleaner if you remove the stopwords and the punctuation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxKy8EzoRkCx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZdITFSK0NFr"
   },
   "source": [
    "It's possible to measure **word** and **sentence similarity** using the SpaCy library.<br> The similarity is based on the word vectors included in the SpaCy model.<br> Each Token object from the spaCy Doc objects contains a word vector representation that can be used to compute similarity.\n",
    "\n",
    "Similiarity can be computed using the **`.similarity()`** method.<br> It can be called on Token objects for word similarity, or Doc objects for sentence similarity:\n",
    "\n",
    "\n",
    "```\n",
    "word_sim = token_a.similarity(token_b)\n",
    "sentence_sim = doc_a.similarity(doc_b)\n",
    "```\n",
    "\n",
    "\n",
    "**3. Compare each token of the following sentence using the `.similarity()` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1D0QigFZyPDB"
   },
   "outputs": [],
   "source": [
    "sentence = \"She's very excited about starting her new job next week.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Fw4nw3oRkta"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldkjtJNX6Yot"
   },
   "source": [
    "**4. Compute the similarity between the four following sentences.<br> Which one seems to be the closest to the first one ?**\n",
    "\n",
    "*Tips: Removing the stopwords will improve the sentence comparison.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60WLaD5P6lEG"
   },
   "outputs": [],
   "source": [
    "sentences = [\"She's very excited about starting her new job next week.\",\n",
    "             \"She's really looking forward to beginning her new role in the coming week.\",\n",
    "             \"Eating apples and oranges daily improves your health.\",\n",
    "             \"The runner was running in a running event. He outran all the other competitors.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTd9shP1RlaK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5vEsbbhzGrR"
   },
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ifg5lpB0fBZ",
    "outputId": "1a5ada49-ba39-492f-efb4-a966c55d16c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       What are the papers written by the person Wazi...\n",
       "1                 What is the Wikidata ID of Yvo Desmedt?\n",
       "2       What is the primary affiliation of Leandro Kru...\n",
       "3                      What is the webpage of Ravi Kumar?\n",
       "4          Which publications did Lambe, Larry A. author?\n",
       "                              ...                        \n",
       "9995    Who has published the most papers among the au...\n",
       "9996    Mention the author who has published the most ...\n",
       "9997    Who published their first paper among the auth...\n",
       "9998    Between 'Graphics and Security: Exploring Visu...\n",
       "9999    Name the author who published their first pape...\n",
       "Name: question, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import ast\n",
    "\n",
    "df = fetch_openml(\"DBLP-QuAD\", parser='auto')['data']['question'].apply(lambda x:ast.literal_eval(x)['string'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUvVQ82UJOym"
   },
   "source": [
    "**Bag of words** can be implement using **Scikit-Learn**'s **`CountVectorizer`**, which converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "**`CountVectorizer`** follows the regular Sklearn syntax:\n",
    "```\n",
    "vectorizer = CountVectorizer()\n",
    "bag_of_words = vectorizer.fit_transform(documents)\n",
    "```\n",
    "It can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). <br>It has several parameters that allow you to customize its behavior:\n",
    "\n",
    "\n",
    "*   **`max_features`**: This parameter allows you to set a maximum number of words for the vocabulary, keeping only the top max_features most frequent words. This can be useful if you have very large documents and want to limit the complexity of the model.\n",
    "\n",
    "*   **`min_df`**: This is the minimum document frequency for a word to be included in the vocabulary. This can be an absolute count (e.g., min_df=5 means a word must appear in at least 5 documents to be included) or a proportion of the total documents (e.g., min_df=0.01 means the word must appear in at least 1% of the documents).\n",
    "\n",
    "*   **`max_df`**: This is the maximum document frequency for a word to be included in the vocabulary. Like min_df, this can be an absolute count or a proportion of total documents. Words that appear too frequently (like common stopwords) can be removed by setting this parameter to a lower value.\n",
    "\n",
    "*   **`stop_words`**: This can be a list of words to be removed from the documents or the string 'english' to use a built-in list of English stop words.\n",
    "\n",
    "*   **`ngram_range`**: This is a tuple (min_n, max_n) that defines the minimum and maximum length of sequences of words to consider. For example, ngram_range=(1, 2) would consider both individual words and pairs of consecutive words.\n",
    "\n",
    "*   **`binary`**: If set to True, this parameter will make the vectorizer use binary occurrence markers instead of counts. This means that the presence or absence of a word will be marked by 1 and 0, regardless of how many times it appears.\n",
    "\n",
    "The output is a sparse matrix where each row represents a specific text in documents and each column represents a word in the vocabulary. <br>To transform the sparse matrix into an array and see the vocabulary you can use:\n",
    "\n",
    "```\n",
    "# The name of the words found in the documents\n",
    "vectorizer.get_feature_names_out()\n",
    "\n",
    "# Transform the sparse matrix into an array\n",
    "bag_of_words.toarray()\n",
    "```\n",
    "\n",
    "\n",
    "**1. Count the word frequency for each text of the dataset using `binary=true` and `stop_words='english'` in `CountVectorizer`.<br>Transform the sparse matrix into an array.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXf_ols4Rmok"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9xKAkLjMVeY"
   },
   "source": [
    "**2. Turn the result in a Pandas Dataframe and add the word names as columns using `.get_feature_names_out()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPO5IszIRnJa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIwxWKjVMt1D"
   },
   "source": [
    "**3. Find the top-5 most frequent words in the documents.**\n",
    "\n",
    "*Tips: You can compare the mean of frequency in the documents for each word using the `.mean()` method and sort the values using the `sort_values()` method.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqFuzDrSRn4q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCrhP1TKNsGO"
   },
   "source": [
    "**4. Find the bi-grams to the bag of words using the `ngram_range` parameter.<br> How many elements does the bag of words have ?**\n",
    "\n",
    "*Tips: A bi-gram is a sequence of two adjacent words  that appear together in a sentence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKRdvRYLRoYU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_vlcEGiPBAL"
   },
   "source": [
    "**5. Find the top-5 most frequent bi-grams in the documents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtWBVN8aRous"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERpnd2iFzIoR"
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koooHSp-oBNk"
   },
   "source": [
    "**TF-IDF** can be implement using **Scikit-Learn**'s **`TfidfVectorizer`**, which converts a collection of text documents to a matrix of TF-IDF features that reflects how important a word is to a document in a collection of documents.\n",
    "\n",
    "**`TfidfVectorizer`** follows the regular Sklearn syntax:\n",
    "\n",
    "```\n",
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf = vectorizer.fit_transform(documents)\n",
    "```\n",
    "\n",
    "It can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). <br>It has several parameters that allow you to customize its behavior:\n",
    "\n",
    "\n",
    "*   **`max_features`**: This parameter allows you to set a maximum number of words for the vocabulary, keeping only the top max_features most frequent words. This can be useful if you have very large documents and want to limit the complexity of the model.\n",
    "\n",
    "*   **`min_df`**: This is the minimum document frequency for a word to be included in the vocabulary. This can be an absolute count (e.g., min_df=5 means a word must appear in at least 5 documents to be included) or a proportion of the total documents (e.g., min_df=0.01 means the word must appear in at least 1% of the documents).\n",
    "\n",
    "*   **`max_df`**: This is the maximum document frequency for a word to be included in the vocabulary. Like min_df, this can be an absolute count or a proportion of total documents. Words that appear too frequently (like common stopwords) can be removed by setting this parameter to a lower value.\n",
    "\n",
    "*   **`stop_words`**: This can be a list of words to be removed from the documents or the string 'english' to use a built-in list of English stop words.\n",
    "\n",
    "*   **`ngram_range`**: This is a tuple (min_n, max_n) that defines the minimum and maximum length of sequences of words to consider. For example, ngram_range=(1, 2) would consider both individual words and pairs of consecutive words.\n",
    "\n",
    "*   **`smooth_idf`**: Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.\n",
    "\n",
    "The output is a sparse matrix where each row represents a specific text in documents and each column represents a word in the vocabulary. <br>To transform the sparse matrix into an array and see the vocabulary you can use:\n",
    "\n",
    "```\n",
    "# The name of the words found in the documents\n",
    "vectorizer.get_feature_names_out()\n",
    "\n",
    "# Transform the sparse matrix into an array\n",
    "tf_idf.toarray()\n",
    "```\n",
    "\n",
    "**1. Compute the tf-idf of each word for each text of the dataset using `TfidfVectorizer`.<br>Transform the sparse matrix into an array.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9CGWew_Rpk4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNBxKJVF8XJ3"
   },
   "source": [
    "**2. Find the word importance of some questions of the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9iyKqpiAYu5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
