{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.2-py3-none-any.whl (5.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (1.0.0)\n",
      "Requirement already satisfied: termcolor in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: toml in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting array-record\n",
      "  Downloading array_record-0.4.0-py38-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 6.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources\n",
      "  Downloading importlib_resources-6.0.0-py3-none-any.whl (31 kB)\n",
      "Collecting protobuf>=3.20\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-macosx_10_9_universal2.whl (400 kB)\n",
      "\u001b[K     |████████████████████████████████| 400 kB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.13.1-py3-none-any.whl (28 kB)\n",
      "Collecting etils[enp,epath]>=0.9.0\n",
      "  Downloading etils-1.3.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dm-tree\n",
      "  Downloading dm_tree-0.1.8-cp38-cp38-macosx_10_9_x86_64.whl (115 kB)\n",
      "\u001b[K     |████████████████████████████████| 115 kB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: psutil in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (5.8.0)\n",
      "Requirement already satisfied: numpy in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (1.22.4)\n",
      "Requirement already satisfied: tqdm in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (4.63.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (2.27.1)\n",
      "Requirement already satisfied: wrapt in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Requirement already satisfied: click in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (8.0.4)\n",
      "Requirement already satisfied: zipp in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (3.7.0)\n",
      "Requirement already satisfied: typing_extensions in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3)\n",
      "Requirement already satisfied: six in /Users/skyemalfoy/opt/anaconda3/lib/python3.8/site-packages (from absl-py->tensorflow_datasets) (1.16.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.59.1-py2.py3-none-any.whl (224 kB)\n",
      "\u001b[K     |████████████████████████████████| 224 kB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=af09c2d684a6571c46a3061262e9225f48513e18662263dfc6611341fb0ad097\n",
      "  Stored in directory: /Users/skyemalfoy/Library/Caches/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built promise\n",
      "Installing collected packages: etils, protobuf, importlib-resources, googleapis-common-protos, tensorflow-metadata, promise, dm-tree, array-record, tensorflow-datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.9.1 requires flatbuffers<2,>=1.12, but you have flatbuffers 2.0 which is incompatible.\n",
      "tensorflow 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.4 which is incompatible.\n",
      "streamlit 1.10.0 requires protobuf<4,>=3.12, but you have protobuf 4.23.4 which is incompatible.\u001b[0m\n",
      "Successfully installed array-record-0.4.0 dm-tree-0.1.8 etils-1.3.0 googleapis-common-protos-1.59.1 importlib-resources-6.0.0 promise-2.3 protobuf-4.23.4 tensorflow-datasets-4.9.2 tensorflow-metadata-1.13.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Users/skyemalfoy/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "S_2Ay0Fp6Bbs"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2CIb2eM9BEx"
   },
   "source": [
    "Definition of a plot function for training result visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_C-ARKUf6Zs4"
   },
   "outputs": [],
   "source": [
    "def plot_results(history):\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    hist_df.columns=[\"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\"]\n",
    "    hist_df.index = np.arange(1, len(hist_df)+1)\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=2, sharex=True, figsize=(16, 10))\n",
    "    axs[0].plot(hist_df.val_accuracy, lw=3, label='Validation Accuracy')\n",
    "    axs[0].plot(hist_df.accuracy, lw=3, label='Training Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].grid()\n",
    "    axs[0].legend(loc=0)\n",
    "    axs[1].plot(hist_df.val_loss, lw=3, label='Validation Loss')\n",
    "    axs[1].plot(hist_df.loss, lw=3, label='Training Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].grid()\n",
    "    axs[1].legend(loc=0)\n",
    "    \n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEGjlF9S9jJ9"
   },
   "source": [
    "## Preprocessing of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvHI_pLN9s2L"
   },
   "source": [
    "We get the IMDB dataset directly from the tensorflow_datasets API and we do the usual preprocessing before feeding a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pLr7YiH96eB2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-22 17:41:41.200536: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /Users/skyemalfoy/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d811b05686cc4f208e3f83af337df4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb386fcb52144fcdb777f4786bc7ba02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /Users/skyemalfoy/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete2X4R5M/imdb_reviews-tr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /Users/skyemalfoy/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete2X4R5M/imdb_reviews-te…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /Users/skyemalfoy/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete2X4R5M/imdb_reviews-un…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /Users/skyemalfoy/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-22 17:42:30.061874: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "batch_size = 32\n",
    "\n",
    "train_set = datasets[\"train\"].shuffle(10000).repeat().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_size = info.splits[\"test\"].num_examples\n",
    "test_set = datasets[\"test\"].repeat().batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxitmaEZA64u"
   },
   "source": [
    "## Use of a pretrained embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjjSBh_H-LRA"
   },
   "source": [
    "We use of pretrained embedding directly from tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zU7z0p_V7O7A",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ab7EQzFmAXmu"
   },
   "source": [
    "We test on two (famous) lines and check the shapes of the embedding results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YQR2XvIk7S03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.03275988  0.18106811  0.13030443  0.05100623  0.12367279 -0.11072872\n",
      "   0.1655957  -0.0049278  -0.3281556   0.05204761  0.17150185  0.01282718\n",
      "  -0.09332222  0.1672171  -0.05711355 -0.22492586 -0.15962309 -0.00958291\n",
      "  -0.11166596 -0.42931503 -0.0194127  -0.20494537  0.25295272  0.05954154\n",
      "  -0.25411132  0.12579551 -0.16218384 -0.10604351  0.27133545 -0.15765025\n",
      "  -0.31424785  0.21318786 -0.10896667  0.14070608 -0.24665987  0.1579746\n",
      "   0.24865562  0.04819695  0.10051076 -0.24969979  0.15491936 -0.0360333\n",
      "   0.07346644  0.10915987 -0.08220651  0.12550174  0.16840625 -0.01693668\n",
      "   0.0715794  -0.04162662]\n",
      " [ 0.16800539  0.24028125 -0.30071175  0.07007764 -0.18024668  0.07986181\n",
      "   0.05427119 -0.28110817 -0.22582981  0.26624134  0.13623291 -0.11988997\n",
      "   0.16064322 -0.04873525 -0.08858649 -0.15337813  0.00109797 -0.26315662\n",
      "   0.3372981  -0.14884004  0.17933601 -0.12853579 -0.15982151 -0.10252967\n",
      "  -0.03884843  0.08044805 -0.20275603 -0.17167023  0.20971875 -0.12899558\n",
      "   0.07407399  0.237014    0.14208776 -0.33200854 -0.12605904 -0.07594581\n",
      "   0.02684807  0.00552503  0.09870575 -0.2602448   0.21202575 -0.0655203\n",
      "  -0.14746019 -0.1037299  -0.02173514  0.0199495  -0.1951208   0.1663805\n",
      "   0.06281476  0.05411559]], shape=(2, 50), dtype=float32)\n",
      "(2, 50)\n"
     ]
    }
   ],
   "source": [
    "embeddings = embed([\"A thing of beauty is a joy forever\", \"If by dull rhymes our English must be chain'd\"])\n",
    "print(embeddings)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXJvUiwrA-4i"
   },
   "source": [
    "## Neural network model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5t7Et6bMGoz4"
   },
   "source": [
    "Build a neural network using keras sequential layers\n",
    "\n",
    "(you may have a look at https://keras.io/api/layers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kobUaul57W0y"
   },
   "outputs": [],
   "source": [
    "# Question 1: Build a neural network using relevant layers, dimensions and activation function (the input layer is already defined to help you)\n",
    "model = tf.keras.models.Sequential([\n",
    "    hub.KerasLayer(embed,\n",
    "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    #??????\n",
    "    #??????\n",
    "    #....\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUFjZK83Gs99"
   },
   "source": [
    "We check that everything is fine with the model as we defined it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7eZA1saGuXa"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kg70hqlPJbaU"
   },
   "source": [
    "We compile the model, choosing the relevant loss function, optimizer and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkIzJDn9G0Av"
   },
   "source": [
    "(You may have a look at\n",
    "https://keras.io/api/losses/\n",
    "and\n",
    "https://keras.io/api/optimizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYIi8jps7aFa"
   },
   "outputs": [],
   "source": [
    "# Question 2: Choose a relevant loss fonction and optimizer for the training\n",
    "loss_function = # ?????\n",
    "optimizer = # ??????\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siQfzR5oHHni"
   },
   "source": [
    "We train the model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6TTGWXb7eEn"
   },
   "outputs": [],
   "source": [
    "# Question 3: Choose relevant values for epochs\n",
    "# (Start with small values for epochs in order to save some computation time)\n",
    "epochs = # ?????\n",
    "\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=epochs, validation_data=test_set, validation_steps=test_size // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h15OAnNqBdP4"
   },
   "source": [
    "## Result visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXjErDMg7lTD"
   },
   "outputs": [],
   "source": [
    "plot_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgshsvFRKC4_"
   },
   "outputs": [],
   "source": [
    "# Question 4: What can you tell about the results? Does it seem satisfying to you? Do you see any hint of an over-fitting? If yes, what kind of layers can you use into the Keras model in order to prevent this phenomenon?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNTWkAU4sFCEYvQ0IAY4bQQ",
   "name": "course2_sentiment_analysis_nn_training_ex.ipynb",
   "provenance": [
    {
     "file_id": "1UcqV7dVM8DE7kCjCTwiBtXaNBm1pkeVr",
     "timestamp": 1622912798930
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
