{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "course3_text_generation_LSTM_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9Bzo0D29mj-"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras.utils as ku \n",
        "import numpy as np \n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti5Q5Xx9Qhxy"
      },
      "source": [
        "Definition of a plot function for training result visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q61YBz5XQxmt"
      },
      "source": [
        "## Data preproccesing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjzitxFS_dpA",
        "outputId": "8c54c35e-a541-42a8-b461-929aaacc9384"
      },
      "source": [
        "# Mounting the google drive to google colab in order to load the data files directly from it\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orUto5KgQ02i"
      },
      "source": [
        "We load a txt file containing Shakespeare sonnets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBPyDBnSEWm0"
      },
      "source": [
        "data = open('/content/drive/MyDrive/EPITA_NLP/Course3/sonnets.txt').read()\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IAn1hV1Seoh"
      },
      "source": [
        "We need to transform each text sentence into token sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh9kumfjSqIF"
      },
      "source": [
        "Then we need to generate from each token sequence, several token subsequences in order to augment the dataset. **Remember** that we want to learn how to predict the next word of a sentence. A sentence of 5 words can so be used to generate 4 training sequences.\n",
        "\n",
        "e.g. the sentence \"*to be or not to be*\" can give the following training sequences: \n",
        "\n",
        "\"*to* **be**\"\n",
        "\n",
        "\"*to be* **or**\"\n",
        "\n",
        "\"*to be or* **not**\"\n",
        "\n",
        "\"*to be or not* **to**\"\n",
        "\n",
        "\"*to be or not to* **be**\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-75oYENbEp3x"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# create input sequences using list of tokens\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "#--- create predictors and label\n",
        "# The predictors correspond to the input sequence without the last token\n",
        "# the label corresponds to the last token (the one we want to predict)\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl2YmmuUXKee",
        "outputId": "a3ff021e-f263-4a74-9037-b6e32e6a2637"
      },
      "source": [
        "sequence_test = tokenizer.texts_to_sequences([\"to be or not to be\"])[0]\n",
        "sequence_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 21, 36, 15, 3, 21]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ1Pd8pJXU3I",
        "outputId": "5673d50d-d272-475d-fec9-7de6d0fb9237"
      },
      "source": [
        "pad_sequences([sequence_test], maxlen=max_sequence_len, padding='post')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3, 21, 36, 15,  3, 21,  0,  0,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNi5IQ1rVrw1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97295c1d-c510-4f2a-9f2d-4ba5e5a03dcb"
      },
      "source": [
        "label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 417,  877,  166, ..., 3210,   15,   14], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXnkNk_3qhtw"
      },
      "source": [
        "## Neural network model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNktnNfFqi16"
      },
      "source": [
        "Build a neural network using at least one LSTM layer\n",
        "\n",
        "(you may have a look at https://keras.io/api/layers/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxEWvq3yE3M1",
        "outputId": "7951a653-8150-4fbf-e2d6-676ba7d2627c"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 10, 100)           321100    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 10, 300)           301200    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 10, 300)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1605)              162105    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3211)              5156866   \n",
            "=================================================================\n",
            "Total params: 6,101,671\n",
            "Trainable params: 6,101,671\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhTePq5sE5TC",
        "outputId": "d6bf8cfe-ea4b-4105-ffec-00601f91f24e"
      },
      "source": [
        " history = model.fit(predictors, label, epochs=100, verbose=1, validation_split=0.1 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "435/435 [==============================] - 49s 103ms/step - loss: 6.9220 - accuracy: 0.0216 - val_loss: 6.7533 - val_accuracy: 0.0220\n",
            "Epoch 2/100\n",
            "435/435 [==============================] - 44s 100ms/step - loss: 6.4866 - accuracy: 0.0241 - val_loss: 6.9354 - val_accuracy: 0.0220\n",
            "Epoch 3/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 6.3890 - accuracy: 0.0238 - val_loss: 6.9683 - val_accuracy: 0.0155\n",
            "Epoch 4/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 6.2765 - accuracy: 0.0295 - val_loss: 7.0989 - val_accuracy: 0.0213\n",
            "Epoch 5/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 6.1792 - accuracy: 0.0357 - val_loss: 7.2174 - val_accuracy: 0.0388\n",
            "Epoch 6/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 6.1025 - accuracy: 0.0379 - val_loss: 7.3718 - val_accuracy: 0.0356\n",
            "Epoch 7/100\n",
            "435/435 [==============================] - 44s 100ms/step - loss: 6.0297 - accuracy: 0.0421 - val_loss: 7.5063 - val_accuracy: 0.0317\n",
            "Epoch 8/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 5.9617 - accuracy: 0.0439 - val_loss: 7.6944 - val_accuracy: 0.0401\n",
            "Epoch 9/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 5.8826 - accuracy: 0.0485 - val_loss: 7.8513 - val_accuracy: 0.0284\n",
            "Epoch 10/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 5.7978 - accuracy: 0.0521 - val_loss: 8.0597 - val_accuracy: 0.0304\n",
            "Epoch 11/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 5.7060 - accuracy: 0.0581 - val_loss: 7.9665 - val_accuracy: 0.0427\n",
            "Epoch 12/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 5.5974 - accuracy: 0.0632 - val_loss: 8.1532 - val_accuracy: 0.0394\n",
            "Epoch 13/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 5.4892 - accuracy: 0.0687 - val_loss: 8.3189 - val_accuracy: 0.0323\n",
            "Epoch 14/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 5.3840 - accuracy: 0.0753 - val_loss: 8.5440 - val_accuracy: 0.0414\n",
            "Epoch 15/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 5.2728 - accuracy: 0.0821 - val_loss: 8.6187 - val_accuracy: 0.0427\n",
            "Epoch 16/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 5.1643 - accuracy: 0.0884 - val_loss: 8.8754 - val_accuracy: 0.0375\n",
            "Epoch 17/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 5.0685 - accuracy: 0.0972 - val_loss: 8.9899 - val_accuracy: 0.0407\n",
            "Epoch 18/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 4.9661 - accuracy: 0.1043 - val_loss: 9.4340 - val_accuracy: 0.0362\n",
            "Epoch 19/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 4.8680 - accuracy: 0.1117 - val_loss: 9.6620 - val_accuracy: 0.0414\n",
            "Epoch 20/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 4.7682 - accuracy: 0.1221 - val_loss: 9.8826 - val_accuracy: 0.0452\n",
            "Epoch 21/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 4.6730 - accuracy: 0.1322 - val_loss: 10.2592 - val_accuracy: 0.0381\n",
            "Epoch 22/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 4.5692 - accuracy: 0.1373 - val_loss: 10.6133 - val_accuracy: 0.0472\n",
            "Epoch 23/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 4.4769 - accuracy: 0.1480 - val_loss: 11.0187 - val_accuracy: 0.0388\n",
            "Epoch 24/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 4.3754 - accuracy: 0.1583 - val_loss: 11.4344 - val_accuracy: 0.0388\n",
            "Epoch 25/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 4.2751 - accuracy: 0.1693 - val_loss: 11.9327 - val_accuracy: 0.0375\n",
            "Epoch 26/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 4.1829 - accuracy: 0.1792 - val_loss: 12.1599 - val_accuracy: 0.0291\n",
            "Epoch 27/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 4.0832 - accuracy: 0.1932 - val_loss: 12.5091 - val_accuracy: 0.0375\n",
            "Epoch 28/100\n",
            "435/435 [==============================] - 44s 100ms/step - loss: 3.9863 - accuracy: 0.2061 - val_loss: 12.7622 - val_accuracy: 0.0297\n",
            "Epoch 29/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 3.8888 - accuracy: 0.2239 - val_loss: 13.1554 - val_accuracy: 0.0284\n",
            "Epoch 30/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 3.7897 - accuracy: 0.2374 - val_loss: 13.8977 - val_accuracy: 0.0336\n",
            "Epoch 31/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 3.7067 - accuracy: 0.2534 - val_loss: 14.1668 - val_accuracy: 0.0284\n",
            "Epoch 32/100\n",
            "435/435 [==============================] - 44s 100ms/step - loss: 3.6066 - accuracy: 0.2709 - val_loss: 14.5778 - val_accuracy: 0.0317\n",
            "Epoch 33/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 3.5174 - accuracy: 0.2927 - val_loss: 15.1649 - val_accuracy: 0.0356\n",
            "Epoch 34/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 3.4266 - accuracy: 0.3060 - val_loss: 15.3337 - val_accuracy: 0.0317\n",
            "Epoch 35/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 3.3418 - accuracy: 0.3277 - val_loss: 16.1606 - val_accuracy: 0.0343\n",
            "Epoch 36/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 3.2521 - accuracy: 0.3494 - val_loss: 16.1827 - val_accuracy: 0.0304\n",
            "Epoch 37/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 3.1680 - accuracy: 0.3643 - val_loss: 16.5507 - val_accuracy: 0.0304\n",
            "Epoch 38/100\n",
            "435/435 [==============================] - 44s 100ms/step - loss: 3.0813 - accuracy: 0.3856 - val_loss: 16.9495 - val_accuracy: 0.0297\n",
            "Epoch 39/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 3.0073 - accuracy: 0.4055 - val_loss: 17.2875 - val_accuracy: 0.0317\n",
            "Epoch 40/100\n",
            "435/435 [==============================] - 45s 104ms/step - loss: 2.9263 - accuracy: 0.4271 - val_loss: 17.4945 - val_accuracy: 0.0271\n",
            "Epoch 41/100\n",
            "435/435 [==============================] - 45s 104ms/step - loss: 2.8623 - accuracy: 0.4430 - val_loss: 18.0151 - val_accuracy: 0.0304\n",
            "Epoch 42/100\n",
            "435/435 [==============================] - 45s 104ms/step - loss: 2.7797 - accuracy: 0.4563 - val_loss: 18.2758 - val_accuracy: 0.0278\n",
            "Epoch 43/100\n",
            "435/435 [==============================] - 45s 104ms/step - loss: 2.7166 - accuracy: 0.4748 - val_loss: 18.3010 - val_accuracy: 0.0259\n",
            "Epoch 44/100\n",
            "435/435 [==============================] - 45s 104ms/step - loss: 2.6561 - accuracy: 0.4886 - val_loss: 18.4949 - val_accuracy: 0.0291\n",
            "Epoch 45/100\n",
            "435/435 [==============================] - 45s 103ms/step - loss: 2.5927 - accuracy: 0.5034 - val_loss: 18.8274 - val_accuracy: 0.0310\n",
            "Epoch 46/100\n",
            "435/435 [==============================] - 45s 104ms/step - loss: 2.5308 - accuracy: 0.5161 - val_loss: 18.9181 - val_accuracy: 0.0271\n",
            "Epoch 47/100\n",
            "435/435 [==============================] - 45s 104ms/step - loss: 2.4724 - accuracy: 0.5308 - val_loss: 19.2936 - val_accuracy: 0.0278\n",
            "Epoch 48/100\n",
            "435/435 [==============================] - 45s 103ms/step - loss: 2.4201 - accuracy: 0.5413 - val_loss: 19.3819 - val_accuracy: 0.0284\n",
            "Epoch 49/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 2.3623 - accuracy: 0.5613 - val_loss: 19.4356 - val_accuracy: 0.0213\n",
            "Epoch 50/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 2.3051 - accuracy: 0.5702 - val_loss: 19.7237 - val_accuracy: 0.0265\n",
            "Epoch 51/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 2.2495 - accuracy: 0.5820 - val_loss: 19.9576 - val_accuracy: 0.0220\n",
            "Epoch 52/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 2.2081 - accuracy: 0.5945 - val_loss: 19.9631 - val_accuracy: 0.0252\n",
            "Epoch 53/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 2.1578 - accuracy: 0.6021 - val_loss: 19.9896 - val_accuracy: 0.0252\n",
            "Epoch 54/100\n",
            "435/435 [==============================] - 44s 100ms/step - loss: 2.1145 - accuracy: 0.6124 - val_loss: 20.1927 - val_accuracy: 0.0239\n",
            "Epoch 55/100\n",
            "435/435 [==============================] - 44s 100ms/step - loss: 2.0846 - accuracy: 0.6221 - val_loss: 20.4458 - val_accuracy: 0.0239\n",
            "Epoch 56/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 2.0349 - accuracy: 0.6245 - val_loss: 20.6225 - val_accuracy: 0.0265\n",
            "Epoch 57/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.9834 - accuracy: 0.6431 - val_loss: 20.8327 - val_accuracy: 0.0207\n",
            "Epoch 58/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.9319 - accuracy: 0.6576 - val_loss: 20.7810 - val_accuracy: 0.0259\n",
            "Epoch 59/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.9080 - accuracy: 0.6579 - val_loss: 20.8795 - val_accuracy: 0.0271\n",
            "Epoch 60/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.8752 - accuracy: 0.6685 - val_loss: 21.0129 - val_accuracy: 0.0259\n",
            "Epoch 61/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.8485 - accuracy: 0.6714 - val_loss: 20.9689 - val_accuracy: 0.0291\n",
            "Epoch 62/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.8167 - accuracy: 0.6770 - val_loss: 21.1644 - val_accuracy: 0.0259\n",
            "Epoch 63/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.7717 - accuracy: 0.6865 - val_loss: 21.2179 - val_accuracy: 0.0239\n",
            "Epoch 64/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.7358 - accuracy: 0.6939 - val_loss: 21.2996 - val_accuracy: 0.0220\n",
            "Epoch 65/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.7097 - accuracy: 0.6992 - val_loss: 21.2140 - val_accuracy: 0.0271\n",
            "Epoch 66/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.6912 - accuracy: 0.7020 - val_loss: 21.6188 - val_accuracy: 0.0284\n",
            "Epoch 67/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.6503 - accuracy: 0.7140 - val_loss: 21.5179 - val_accuracy: 0.0252\n",
            "Epoch 68/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.6370 - accuracy: 0.7133 - val_loss: 21.3527 - val_accuracy: 0.0246\n",
            "Epoch 69/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.6117 - accuracy: 0.7199 - val_loss: 21.6071 - val_accuracy: 0.0271\n",
            "Epoch 70/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.5803 - accuracy: 0.7266 - val_loss: 21.5629 - val_accuracy: 0.0278\n",
            "Epoch 71/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.5408 - accuracy: 0.7390 - val_loss: 21.5810 - val_accuracy: 0.0259\n",
            "Epoch 72/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.5222 - accuracy: 0.7427 - val_loss: 21.7091 - val_accuracy: 0.0252\n",
            "Epoch 73/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.5028 - accuracy: 0.7454 - val_loss: 21.6466 - val_accuracy: 0.0259\n",
            "Epoch 74/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.4801 - accuracy: 0.7464 - val_loss: 21.5911 - val_accuracy: 0.0252\n",
            "Epoch 75/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.4748 - accuracy: 0.7458 - val_loss: 21.5175 - val_accuracy: 0.0239\n",
            "Epoch 76/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.4365 - accuracy: 0.7547 - val_loss: 21.6452 - val_accuracy: 0.0278\n",
            "Epoch 77/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.4150 - accuracy: 0.7587 - val_loss: 21.9510 - val_accuracy: 0.0259\n",
            "Epoch 78/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.4043 - accuracy: 0.7581 - val_loss: 21.7461 - val_accuracy: 0.0297\n",
            "Epoch 79/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.3911 - accuracy: 0.7600 - val_loss: 21.6461 - val_accuracy: 0.0278\n",
            "Epoch 80/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.3703 - accuracy: 0.7658 - val_loss: 21.6736 - val_accuracy: 0.0246\n",
            "Epoch 81/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.3502 - accuracy: 0.7740 - val_loss: 21.7821 - val_accuracy: 0.0220\n",
            "Epoch 82/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.3294 - accuracy: 0.7725 - val_loss: 21.7502 - val_accuracy: 0.0271\n",
            "Epoch 83/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.3176 - accuracy: 0.7740 - val_loss: 21.7077 - val_accuracy: 0.0259\n",
            "Epoch 84/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.2952 - accuracy: 0.7792 - val_loss: 21.6649 - val_accuracy: 0.0252\n",
            "Epoch 85/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.3025 - accuracy: 0.7757 - val_loss: 21.5816 - val_accuracy: 0.0304\n",
            "Epoch 86/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.2832 - accuracy: 0.7850 - val_loss: 21.9171 - val_accuracy: 0.0278\n",
            "Epoch 87/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.2681 - accuracy: 0.7840 - val_loss: 21.9084 - val_accuracy: 0.0278\n",
            "Epoch 88/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.2456 - accuracy: 0.7878 - val_loss: 21.8225 - val_accuracy: 0.0284\n",
            "Epoch 89/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.2340 - accuracy: 0.7902 - val_loss: 21.7129 - val_accuracy: 0.0246\n",
            "Epoch 90/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.2144 - accuracy: 0.7934 - val_loss: 21.6650 - val_accuracy: 0.0252\n",
            "Epoch 91/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.2012 - accuracy: 0.7975 - val_loss: 21.4981 - val_accuracy: 0.0252\n",
            "Epoch 92/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.1893 - accuracy: 0.7963 - val_loss: 21.7945 - val_accuracy: 0.0284\n",
            "Epoch 93/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.1801 - accuracy: 0.8008 - val_loss: 21.6717 - val_accuracy: 0.0239\n",
            "Epoch 94/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.1798 - accuracy: 0.7958 - val_loss: 21.6970 - val_accuracy: 0.0265\n",
            "Epoch 95/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.1679 - accuracy: 0.7980 - val_loss: 21.6440 - val_accuracy: 0.0246\n",
            "Epoch 96/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.1534 - accuracy: 0.8027 - val_loss: 21.9272 - val_accuracy: 0.0246\n",
            "Epoch 97/100\n",
            "435/435 [==============================] - 44s 101ms/step - loss: 1.1405 - accuracy: 0.8034 - val_loss: 21.7528 - val_accuracy: 0.0259\n",
            "Epoch 98/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.1399 - accuracy: 0.8008 - val_loss: 21.5196 - val_accuracy: 0.0246\n",
            "Epoch 99/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.1112 - accuracy: 0.8108 - val_loss: 21.8065 - val_accuracy: 0.0259\n",
            "Epoch 100/100\n",
            "435/435 [==============================] - 44s 102ms/step - loss: 1.0935 - accuracy: 0.8101 - val_loss: 21.5141 - val_accuracy: 0.0239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAASHMx76TgY"
      },
      "source": [
        "## Result visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbWmhe3Jr5Zz"
      },
      "source": [
        "## Text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0SXL2-Yr7o_"
      },
      "source": [
        "You can now use your model to sequentially generate new words from a given uncomplete sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9i3YkKWF4w0",
        "outputId": "fbd1ca38-18c6-4946-84f4-95ea8aaf6cc4"
      },
      "source": [
        "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted_aux = model.predict(token_list, verbose=0)\n",
        "\tpredicted = np.argmax(predicted_aux, axis=1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Help me Obi Wan Kenobi, you're my only hope of either's pen night worth gentle part rhyme lies impute rage ' and bevel gone so last alone seen shown last alone rage ' doth lust doth share truth dead woe pride ill report skill repair so bright alone see so sort sort bright be growest date prime rage can spend brought still none toil'd lie still seen seen remain bright doth approve another too ill skill skill report ' rage of wrong ' must untold untold alone alone now lust new who lie remain alone doth bold ' hath die my barren rhyme ' doth hell to live alone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-Cbe0GLF7-p"
      },
      "source": [
        "Reference:\n",
        "\n",
        "https://www.coursera.org/learn/natural-language-processing-tensorflow"
      ]
    }
  ]
}