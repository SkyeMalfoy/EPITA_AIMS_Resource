{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"course3_text_sequence_preprocessing_ex.ipynb","provenance":[{"file_id":"1zqvtwXUXfZ9i1K32X27jMlxKp77adwMU","timestamp":1623881753712}],"authorship_tag":"ABX9TyM3gW8BWolvTHES6u1yCM5k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-urn6K-R4zoQ"},"source":["This notebook serves as a training for the basics text sequence preprocessing to do in order to train RNN or LSTM networks"]},{"cell_type":"code","metadata":{"id":"2aVhL2fkrMH6","executionInfo":{"status":"ok","timestamp":1623881781429,"user_tz":-120,"elapsed":15,"user":{"displayName":"Romain Benassi","photoUrl":"","userId":"07755091784567448642"}}},"source":["import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"37ZZ8UCdrd-u"},"source":["## Preprocessing of the data"]},{"cell_type":"markdown","metadata":{"id":"Liw-_SS2roy4"},"source":["We get the IMDB dataset directly from the tensorflow_datasets API"]},{"cell_type":"code","metadata":{"id":"sUcOr9j9rn2T"},"source":["import tensorflow_datasets as tfds\n","\n","datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"57N_uRFcr36c"},"source":["We reformate the training data to make easier the preprocessing wanted"]},{"cell_type":"code","metadata":{"id":"73mPIRanr8Vz","executionInfo":{"status":"ok","timestamp":1623881847105,"user_tz":-120,"elapsed":6620,"user":{"displayName":"Romain Benassi","photoUrl":"","userId":"07755091784567448642"}}},"source":["train_data = datasets['train']\n","\n","training_sentences = []\n","training_labels = []\n","\n","for s,l in train_data:\n","  training_sentences.append(str(s.numpy()))\n","  training_labels.append(l.numpy())\n","  \n","training_labels_final = np.array(training_labels)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NRUA44hC2o9o"},"source":["## Tokenization"]},{"cell_type":"markdown","metadata":{"id":"_kMGeUwWs2Ge"},"source":["Use of the Tokenizer object from Keras to tokenize the sequence.\n","\n","(You can have a look at the doc page https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)"]},{"cell_type":"code","metadata":{"id":"k7dvn-SEs0f1","executionInfo":{"status":"ok","timestamp":1623881854804,"user_tz":-120,"elapsed":7703,"user":{"displayName":"Romain Benassi","photoUrl":"","userId":"07755091784567448642"}}},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","tokenizer = Tokenizer()\n","\n","# Question 1: use the \"tokenizer\" object to transform the original texts into tokenized sequences \n","\n","# tokenized_sequences = ??????\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L719Mlcp2h9B"},"source":["We can check if we really get a sequence of tokens"]},{"cell_type":"code","metadata":{"id":"9_I7zq3iu6Ib"},"source":["print(tokenized_sequences[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BBj9JOiN3Mg5"},"source":["We can have a look at the explicit correspondance between words and token numbers"]},{"cell_type":"code","metadata":{"id":"EZ6PyspLyNKt"},"source":["tokenizer.word_index"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tZi1bpPY2ryU"},"source":["## Padding"]},{"cell_type":"markdown","metadata":{"id":"VqJE7JszvM1z"},"source":["Use of the pad_sequences function to pad the tokenized sequences created just before.\n","\n","(See https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)"]},{"cell_type":"code","metadata":{"id":"g1pwNBZ5u8zi","executionInfo":{"status":"ok","timestamp":1623881855748,"user_tz":-120,"elapsed":631,"user":{"displayName":"Romain Benassi","photoUrl":"","userId":"07755091784567448642"}}},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Question 2: use the \"pad_sequences\" function to transform the tokenized sequences into padded sequences\n","\n","#max_length = ?????? # choose a length value the padded sequences\n","#padded_sequences = ??????"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JzfHT5E_3Vs9"},"source":["We can check if we really get a padded sequence of tokens"]},{"cell_type":"code","metadata":{"id":"kSs0QBMnvrcM"},"source":["print(padded_sequences[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ffeV-PSP3doQ"},"source":["A quick comparison between the padded and the original sequences"]},{"cell_type":"code","metadata":{"id":"3w_DiAkdv33i"},"source":["padded_sequence_length = len(padded_sequences[0])\n","original_sequence_length = len(tokenized_sequences[0])\n","\n","print(f\"The length of the padded sequence is {padded_sequence_length} whereas it used to be {original_sequence_length} before padding\\n\")\n","\n","if padded_sequence_length>original_sequence_length:\n","  print(\"You should observed zero-padding on the new sequence\")\n","else:\n","    print(\"You should observed a troncation of the original sequence\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aHoYdu2m2u0X"},"source":["## One-hot encoding"]},{"cell_type":"markdown","metadata":{"id":"ed1m-5dv9pJC"},"source":["Use of the to_categorical function to convert each token of a sequence into its one-hot encoded version\n","\n","(See https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical)"]},{"cell_type":"markdown","metadata":{"id":"ndJVlXRB2y2v"},"source":["We try only on the first sequence, in order to prevent memory crashing"]},{"cell_type":"code","metadata":{"id":"sTUFZThpxEsF","executionInfo":{"status":"ok","timestamp":1623881855750,"user_tz":-120,"elapsed":8,"user":{"displayName":"Romain Benassi","photoUrl":"","userId":"07755091784567448642"}}},"source":["from tensorflow.keras.utils import to_categorical\n","\n","first_padded_sequence = padded_sequences[0]\n","\n","# Question 3: use the \"to_categorical\" function to transform the first padded sequence (and it only, to prevent memory crash) into a one-hot encoded sequence\n","\n","#one_hot_encoded_first_sequence = ??????"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3rAIad2K3mrc"},"source":["We can check if we really get a sequence of one-hot-encoded tokens"]},{"cell_type":"code","metadata":{"id":"CVixwLysxXES"},"source":["print(one_hot_encoded_first_sequence)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ZviXYKO4DZZ"},"source":["As we saw in the class, most of the time we will not need to explictly compute the one-hot encoded sequence vectors"]},{"cell_type":"markdown","metadata":{"id":"wUCI0Ozw4TB1"},"source":["## Reverse processing: from the padded sequences to the original text sequences"]},{"cell_type":"markdown","metadata":{"id":"G1UthLr74jJP"},"source":["We define a function to get the original sequences back from the padded ones"]},{"cell_type":"code","metadata":{"id":"iUQxScMXx3BC"},"source":["# Question 4: Define a function to get the original sequences back from the padded ones\n","\n","# ??????"],"execution_count":null,"outputs":[]}]}