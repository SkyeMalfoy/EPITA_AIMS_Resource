{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2259,
     "status": "ok",
     "timestamp": 1623891557725,
     "user": {
      "displayName": "Romain Benassi",
      "photoUrl": "",
      "userId": "07755091784567448642"
     },
     "user_tz": -120
    },
    "id": "m9Bzo0D29mj-"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.utils as ku \n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ti5Q5Xx9Qhxy"
   },
   "source": [
    "Definition of a plot function for training result visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1623891561124,
     "user": {
      "displayName": "Romain Benassi",
      "photoUrl": "",
      "userId": "07755091784567448642"
     },
     "user_tz": -120
    },
    "id": "rVKoPUqJ_apr"
   },
   "outputs": [],
   "source": [
    "def plot_results(history):\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    hist_df.columns=[\"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\"]\n",
    "    hist_df.index = np.arange(1, len(hist_df)+1)\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=2, sharex=True, figsize=(16, 10))\n",
    "    axs[0].plot(hist_df.val_accuracy, lw=3, label='Validation Accuracy')\n",
    "    axs[0].plot(hist_df.accuracy, lw=3, label='Training Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].grid()\n",
    "    axs[0].legend(loc=0)\n",
    "    axs[1].plot(hist_df.val_loss, lw=3, label='Validation Loss')\n",
    "    axs[1].plot(hist_df.loss, lw=3, label='Training Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].grid()\n",
    "    axs[1].legend(loc=0)\n",
    "    \n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q61YBz5XQxmt"
   },
   "source": [
    "## Data preproccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19391,
     "status": "ok",
     "timestamp": 1623891582851,
     "user": {
      "displayName": "Romain Benassi",
      "photoUrl": "",
      "userId": "07755091784567448642"
     },
     "user_tz": -120
    },
    "id": "mjzitxFS_dpA",
    "outputId": "5f6cbada-c2a1-4bc2-ca28-7047c0b7b900"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Mounting the google drive to google colab in order to load the data files directly from it\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Mounting the google drive to google colab in order to load the data files directly from it\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orUto5KgQ02i"
   },
   "source": [
    "We load a txt file containing Shakespeare sonnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 967,
     "status": "ok",
     "timestamp": 1623891587482,
     "user": {
      "displayName": "Romain Benassi",
      "photoUrl": "",
      "userId": "07755091784567448642"
     },
     "user_tz": -120
    },
    "id": "yBPyDBnSEWm0"
   },
   "outputs": [],
   "source": [
    "data = open('sonnets.txt').read()\n",
    "\n",
    "corpus = data.lower().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1623891601968,
     "user": {
      "displayName": "Romain Benassi",
      "photoUrl": "",
      "userId": "07755091784567448642"
     },
     "user_tz": -120
    },
    "id": "ZM7_ViIGe0NW",
    "outputId": "950663a1-b636-4585-e8f3-bda789e0089b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from fairest creatures we desire increase,',\n",
       " \"that thereby beauty's rose might never die,\",\n",
       " 'but as the riper should by time decease,',\n",
       " 'his tender heir might bear his memory:',\n",
       " 'but thou, contracted to thine own bright eyes,',\n",
       " \"feed'st thy light'st flame with self-substantial fuel,\",\n",
       " 'making a famine where abundance lies,',\n",
       " 'thyself thy foe, to thy sweet self too cruel.',\n",
       " \"thou that art now the world's fresh ornament\",\n",
       " 'and only herald to the gaudy spring,']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IAn1hV1Seoh"
   },
   "source": [
    "We need to transform each text sentence into token sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oh9kumfjSqIF"
   },
   "source": [
    "Then we need to generate from each token sequence, several token subsequences in order to augment the dataset. **Remember** that we want to learn how to predict the next word of a sentence. A sentence of 5 words can so be used to generate 4 training sequences.\n",
    "\n",
    "e.g. the sentence \"*to be or not to be*\" can give the following training sequences: \n",
    "\n",
    "\"*to* **be**\"\n",
    "\n",
    "\"*to be* **or**\"\n",
    "\n",
    "\"*to be or* **not**\"\n",
    "\n",
    "\"*to be or not* **to**\"\n",
    "\n",
    "\"*to be or not to* **be**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "-75oYENbEp3x"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Question 1: use the preprocessing steps learned in course3_text_sequence_preprocessing_ex.ipynb to \n",
    "#create the padded sequences needed to train the model\n",
    "# Hint: Be careful about the length you will use for the padding sequences AND about where you put the extra \n",
    "#zero coming from the padding (at the beginning of the sequence or at the end)\n",
    "\n",
    "# Tokenize\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "tokenized_sentences = tokenizer.texts_to_sequences(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "#Padding\n",
    "max_length = 300\n",
    "#Put paddings in the front\n",
    "padded_sentences = pad_sequences(tokenized_sentences, maxlen=max_length, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    " \n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    " \n",
    "# create input sequences using list of tokens\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "#         print(n_gram_sequence)\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "    \n",
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "#--- create predictors and label\n",
    "# The predictors correspond to the input sequence without the last token\n",
    "# the label corresponds to the last token (the one we want to predict)\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXnkNk_3qhtw"
   },
   "source": [
    "## Neural network model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNktnNfFqi16"
   },
   "source": [
    "Build a neural network using at least one LSTM layer\n",
    "\n",
    "(you may have a look at https://keras.io/api/layers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "jxEWvq3yE3M1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 10, 100)           321100    \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 10, 300)          301200    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10, 300)           0         \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 10, 256)           570368    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10, 2)             514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,193,182\n",
      "Trainable params: 1,193,182\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "\n",
    "# Question 2: define a neural network model using at least one LSTM layer\n",
    "# Hint1: we advise you to use as first layer an Embedding layer (have a look at course3_sentiment_analysis_LSTM_ex.ipynb for a reminder of how to use it)\n",
    "# Hint2: you can import any additional layers from tensorflow.keras.layers if needed\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len - 1))\n",
    "model.add(Bidirectional(LSTM(150, activation='tanh', return_sequences=True)))\n",
    "model.add(Dropout(.2))\n",
    "model.add(LSTM(256, activation='tanh', recurrent_activation='sigmoid', return_sequences=True))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# Question 3: define a relevant loss function and optimizer\n",
    "\n",
    "loss_function = 'sparse_categorical_crossentropy'\n",
    "optimizer = Adam()\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optimizer,metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "AhTePq5sE5TC"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_split_value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Question 4: train your model with relevant parameters\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# predictors = ??????\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# label = ??????\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# epochs_value = ??????\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# validation_split_value = ??????\u001b[39;00m\n\u001b[1;32m      7\u001b[0m epochs_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m\n\u001b[0;32m----> 8\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(predictors, label, epochs\u001b[38;5;241m=\u001b[39mepochs_value, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[43mvalidation_split_value\u001b[49m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'validation_split_value' is not defined"
     ]
    }
   ],
   "source": [
    "# Question 4: train your model with relevant parameters\n",
    "\n",
    "# predictors = ??????\n",
    "# label = ??????\n",
    "# epochs_value = ??????\n",
    "# validation_split_value = ??????\n",
    "epochs_value=20\n",
    "history = model.fit(predictors, label, epochs=epochs_value, verbose=1, validation_split=validation_split_value )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAASHMx76TgY"
   },
   "source": [
    "## Result visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgKycPomE_Dc"
   },
   "outputs": [],
   "source": [
    "plot_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbWmhe3Jr5Zz"
   },
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0SXL2-Yr7o_"
   },
   "source": [
    "You can now use your model to sequentially generate new words from a given uncomplete sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9i3YkKWF4w0"
   },
   "outputs": [],
   "source": [
    "seed_text = \"Put here the begining of a sentence of your own\"\n",
    "nb_additional_words = 100\n",
    "  \n",
    "# Question 5: generate nb_additional_words at the end of seed_text according to your trained model\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNQRK4df74efJqVXGlwN6SG",
   "name": "course3_text_generation_LSTM_ex.ipynb",
   "provenance": [
    {
     "file_id": "1W5YJEt9tn5UmAVGXG1iKylxbv76zZpbQ",
     "timestamp": 1623887037576
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
