{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKrYb6F5UmQU"
   },
   "source": [
    "*This Notebook was created by Antoine Palisson*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "S3ud0IgvVEME"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdaKgT1-HeQT"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NLctxy76vdK_"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "data = fetch_openml('artificial-characters', version=1, as_frame=True)\n",
    "X = data['data']\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>46.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>46.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>22.47</td>\n",
       "      <td>46.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>23.41</td>\n",
       "      <td>46.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>35.74</td>\n",
       "      <td>46.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    V1    V2    V3    V4    V5     V6    V7\n",
       "0  0.0   0.0   0.0   0.0  20.0  20.00  46.1\n",
       "1  1.0  19.0   0.0  19.0   8.0   8.00  46.1\n",
       "2  2.0   0.0  20.0  19.0   8.0  22.47  46.1\n",
       "3  3.0   0.0  20.0   8.0  42.0  23.41  46.1\n",
       "4  4.0  19.0   8.0   8.0  42.0  35.74  46.1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: Class, dtype: category\n",
       "Categories (10, object): ['1', '2', '3', '4', ..., '7', '8', '9', '10']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 10218 entries, 0 to 10217\n",
      "Series name: Class\n",
      "Non-Null Count  Dtype   \n",
      "--------------  -----   \n",
      "10218 non-null  category\n",
      "dtypes: category(1)\n",
      "memory usage: 10.5 KB\n"
     ]
    }
   ],
   "source": [
    "y.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdtV_k87HhlQ"
   },
   "source": [
    "# Quick Exploration & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZ1WAy2vHnSX"
   },
   "source": [
    "**<font color='blue'>1.a. How many classes does the label have ?<br>1.b. Is the dataset balanced ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5kTt_nbhyN2o"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     1416\n",
       "8     1198\n",
       "1     1196\n",
       "2     1192\n",
       "5     1008\n",
       "6     1000\n",
       "9     1000\n",
       "4      808\n",
       "7      800\n",
       "10     600\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()\n",
    "# 1.a. It has 10 classes.\n",
    "# 1.b. It is inbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BI_XS_aUIRYc"
   },
   "source": [
    "**<font color='blue'>2. Split the dataset into a training and a testing set.**\n",
    "\n",
    "*Tips: Don't forget to do the splitting according to the type of the task (classification, regression) and the dataset label (balanced or not).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classfication task, and the dataset label is not balanced. So we can do stratify. <br/><br/>\n",
    "The dataset will be splitted into:<br/>\n",
    "(1) 70% training set;<br/>\n",
    "(2) 10% validation set;<br/>\n",
    "(3) 20% testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "K54bnh02yOM4"
   },
   "outputs": [],
   "source": [
    "#Split training and rest\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, train_size=0.7, random_state=42)\n",
    "#Split validation and rest\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, train_size = 1/3, stratify=y_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3     0.138563\n",
      "8     0.117170\n",
      "1     0.117030\n",
      "2     0.116611\n",
      "5     0.098714\n",
      "6     0.097875\n",
      "9     0.097875\n",
      "4     0.079139\n",
      "7     0.078300\n",
      "10    0.058725\n",
      "Name: Class, dtype: float64\n",
      "3     0.138943\n",
      "1     0.117417\n",
      "8     0.117417\n",
      "2     0.116438\n",
      "5     0.098826\n",
      "6     0.097847\n",
      "9     0.097847\n",
      "4     0.078278\n",
      "7     0.078278\n",
      "10    0.058708\n",
      "Name: Class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_val.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7F3lAUJI5Wo"
   },
   "source": [
    "The dataset only contains numerical features.<br>The purpose of this exercise is not to explore the data or to do specific preprocessing.\n",
    "\n",
    "**<font color='blue'>3. How should you preprocess the dataset ?<br> Don't apply the preprocessing yet.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2v-5NsoGyPhb"
   },
   "source": [
    "<b>Preprocessing steps:</b>\n",
    "<li>Missing values processing;</li>\n",
    "<li>Outliers processing;</li>\n",
    "<li>Handle data errors;</li>\n",
    "<li>Duplications processing;</li>\n",
    "<li>For Numerical data: data transformation (scaling or math tranformation)</li>\n",
    "<li>For categorical data: data transformation (according the type of categorical data); <br/>\n",
    "Encoding</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGywz0MqJLqt"
   },
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0auRVP7Mgo7"
   },
   "source": [
    "**<font color='blue'>1.a. Which metric from the sklearn library should you use for this dataset ?<br> Should you change some of its parameters ?**\n",
    "\n",
    "*Tips: Is the dataset balanced ? How many classes does the label ?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvUDaYqEyRNr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTS0tF7fM1xh"
   },
   "source": [
    "The **`make_scorer`** function is a utility function in the Sklearn library that allows you to create a custom scoring function that can be used in model selection and evaluation. It essentially transforms an arbitrary function into a scorer object that can be passed to the `cross_val_score` or `GridSearchCV` functions (you will use them in this notebook).\n",
    "\n",
    "You can find it [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html).\n",
    "\n",
    "---\n",
    "\n",
    "The `make_scorer` **parameters**:\n",
    "\n",
    "* **`score_func`**: This parameter is a function that computes the score for a given set of predicted and true values. The function takes two arguments: the true labels and the predicted labels. The score function can be any function that returns a scalar value, such as accuracy, precision, recall, F1-score, etc.\n",
    "\n",
    "* **`greater_is_better`**: This parameter is a boolean value that determines whether a higher score is better or worse for the model. If set to True, the scorer will be maximizing the score; if set to False, the scorer will be minimizing the score.\n",
    "\n",
    "* **`needs_proba`**: This parameter is a boolean value that determines whether the scorer requires the model to output predicted probabilities instead of predicted labels. If set to True, the scorer expects the model to output probabilities, and the score_func function will be applied to the probabilities instead of the predicted labels.\n",
    "\n",
    "* **`needs_threshold`**: This parameter is a boolean value that determines whether the scorer requires the model to output a probability threshold. If set to True, the scorer will optimize the threshold in addition to the model's hyperparameters.\n",
    "\n",
    "* **`kwargs`**: This parameter is a dictionary of additional keyword arguments that can be passed to the score_func function. These arguments can be used to customize the behavior of the score function, such as changing the weight of different classes or adjusting the threshold for binary classification.\n",
    "\n",
    "---\n",
    "**Code examples**:\n",
    "\n",
    "Example 1 - *it is the same as passing `scoring='accuracy'`*\n",
    "```\n",
    "acc_scorer = make_scorer(accuracy_score)\n",
    "scores = cross_val_score(..., scoring=acc_scorer)\n",
    "```\n",
    "\n",
    "Example 2 - *average is a parameter of the `f1_score` function*\n",
    "```\n",
    "f1_scorer = make_scorer(f1_score, average='micro')\n",
    "scores = cross_val_score(..., scoring=f1_scorer)\n",
    "```\n",
    "\n",
    "\n",
    "**<font color='blue'>1.b. Use the `make_scorer` function from sklearn to create a metric with the correct parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHrTrkTCyR4E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMDdn5MDJOV2"
   },
   "source": [
    "**Let's compare four models:**\n",
    "\n",
    "*   Logistic Regression\n",
    "*   Support Vector Classifier\n",
    "*   k-Neighbors Classifier\n",
    "*   Decision Tree Classifier\n",
    "\n",
    "To compare the models, you will use a **cross-validation method**.\n",
    "\n",
    "As a remainder:\n",
    "\n",
    "> *The **`Pipeline`** class in sklearn is a tool for chaining multiple processing steps together into a single estimator. It can be used to automate the workflow of a machine learning project by **combining data preprocessing and modeling into a single object** that can be used for training and prediction. It can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline).*\n",
    "\n",
    "> *The most important parameter of the Pipeline is the `steps`: it is a list of tuples, where each tuple contains the name of the step and the processing object. The steps are executed in the order they are listed.*\n",
    "\n",
    ">```\n",
    "pipeline = Pipeline(steps=[('preprocesing', StandardScaler()),        # Preprocessing\n",
    "                           ('model', LogisticRegression())])          # Model\n",
    "\n",
    "\n",
    "**<font color='blue'>2.a. Use the `Pipeline` class to merge the preprocessing function and a Logistic Regression model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKisKTjUySqa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1N0kD92KR1W"
   },
   "source": [
    "**<font color='blue'>2.b. Do a cross-validation method to evaluate the model performance using the `make_scorer` function.<br>Show the mean and the standard deviation of the scores.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SatmgIZLyTFE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFmum2wLQ4AU"
   },
   "source": [
    "**<font color='blue'>3. Do the same for the Support Vector Classifier model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxBSoXBryTbH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "to6GNzz1RJOM"
   },
   "source": [
    "**<font color='blue'>4. Do the same for the k-neighbors classifier model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdGc-5QryT2f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqBTY5-nRZIN"
   },
   "source": [
    "**<font color='blue'>5. Finally, do the same for the Decision Tree classifier model.<br>Which model is the best ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNQCrVq9yUl8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwh_c2dBRxOV"
   },
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYzN-ZjuR1kI"
   },
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUFtHaQpR48r"
   },
   "source": [
    "The **Decision Tree classifier** model has many parameters.<br> Let's try to tune the following ones:\n",
    "\n",
    "*   **`criterion`** which can takes three values gini, entropy or logloss\n",
    "*   **`splitter`** which can takes two values random or best\n",
    "*   **`max_depth`** which can any positive integer or None (i.e. infinite)\n",
    "*   **`min_samples_split`** which can any positive integer from 2\n",
    "*   **`min_samples_leaf`** which can any positive integer from 1\n",
    "*   **`max_features`** which can any positive integer from 1 to the number of features\n",
    "\n",
    "Explaining these hyperparameters is out of the scopre of this exercise.<br>Thus, we will consider each of them has potentially very important for the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ATC9Y_WTAUa"
   },
   "source": [
    "**<font color='blue'>1. Change the values of each of the hyperparameters independently and evaluate the model performance.<br>Find the hyperparameters that make the performances change a lot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0huduNcyWnN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6bbB3SXTj3E"
   },
   "source": [
    "The **`GridSearchCV`** class in Sklearn is a tool for performing an exhaustive search over a specified parameter grid for an estimator. It searches over all possible combinations of the parameters to determine the best parameter values based on the chosen evaluation metric.\n",
    "\n",
    "You can find it [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV).\n",
    "\n",
    "---\n",
    "\n",
    "Most important **parameters**:\n",
    "\n",
    "* **`estimator`**: This parameter takes an estimator object that is to be tuned using GridSearchCV. The estimator object should implement a fit method that takes the training data as input.\n",
    "\n",
    "* **`param_grid`**: This parameter is a dictionary or a list of dictionaries that defines the hyperparameter search space. The keys of the dictionary are the hyperparameter names and the values are the corresponding search spaces. A search space is a list of possible values or a distribution where the values are sampled.\n",
    "\n",
    "* **`scoring`**: This parameter specifies the metric to use for evaluating the performance of the model with different hyperparameters. It can take a string representing a built-in scoring metric, a callable object that implements a custom scoring metric with make_scorer, or a list/tuple of multiple scoring metrics.\n",
    "\n",
    "* **`cv`**: This parameter specifies the cross-validation splitting strategy. It can take an integer value representing the number of folds in a KFold cross-validation, a cross-validation iterator, or a specific data splitting strategy. It specifies how the data is partitioned into training and validation sets for each hyperparameter combination.\n",
    "\n",
    "---\n",
    "\n",
    "Most important **attributes**:\n",
    "\n",
    "* **`cv_results_`**: This attribute is a dictionary that contains all of the cross-validation results for each combination of hyperparameters tried during the GridSearchCV search. It includes information such as the mean and standard deviation of the test scores, training times, and hyperparameter values for each combination.\n",
    "\n",
    "* **`best_params_`**: This attribute is a dictionary that contains the best hyperparameter values found during the GridSearchCV search. It includes the hyperparameter names as keys and their corresponding best values as values.\n",
    "\n",
    "* **`best_score_`**: This attribute is a float value that represents the best cross-validation score obtained during the GridSearchCV search.\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "params = {...}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, \n",
    "                           param_grid=params)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "results = grid_search.cv_results_\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**<font color='blue'>2. Create a param_grid dictionnary with a maximum of 5 different values per hyperparameter.<br> How many trials would you perform ?**\n",
    "\n",
    "*Tips: If you are using a pipeline, then the model hyperparameters have been renamed to the name of the step in the Pipeline + two underscores + the name of the hyperparameter (see below).*\n",
    "\n",
    "```\n",
    "pipe = Pipeline([('preprocessing', ...),\n",
    "                 ('model', ...)])\n",
    "params = {\"model__hyperparameter1\" : [...],\n",
    "          \"model__hyperparameter2\" : [...]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvDhwouiyXjb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Y35iHg8ZNJN"
   },
   "source": [
    "**<font color='blue'>2. Use a Grid Search strategy to find the best hyperparameter using the param_grid defined at the previous question.**\n",
    "\n",
    "*Tips: You should pass the Pipeline to the estimator parameter.<br> Additionally, you should pass the custom scorer to the scoring parameter.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSUv26gHyYIT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTfcBKPQdAkV"
   },
   "source": [
    "**<font color='blue'>3.a. Get the result of all the trials using the .cv_results_ attribute and transform it into a DataFrame.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hobhEB8ryYsx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlvTwdLUdPcZ"
   },
   "source": [
    "**<font color='blue'>3.b. Sort the trials by the `rank_test_score` column.<br>What is the best set of hyperparameters ? Is it better than the default model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Evov4WAyZKh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1gGR34BdxZJ"
   },
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8lp-W5ZeC8Q"
   },
   "source": [
    "**`RandomizedSearchCV`** is a class in Scikit-learn that randomly selects a subset of hyperparameters and fits the model using those hyperparameters, repeating this process for a specified number of iterations to find the optimal combination of hyperparameters that produce the best performance on a given metric.\n",
    "\n",
    "You can find it [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV).\n",
    "\n",
    "---\n",
    "\n",
    "**`RandomizedSearchCV`** has very similar parameters to **`GridSearchCV`**.<br> It also adds new parameters suchs as:\n",
    "\n",
    "* **`param_distributions`**: This parameter is a dictionary or a list of dictionaries, where each dictionary contains hyperparameter distributions to be sampled from. The hyperparameters to be tuned are specified as keys in each dictionary, and the corresponding value is a distribution over the hyperparameter space from which to sample. This parameter controls the search space from which the hyperparameters are randomly sampled.\n",
    "\n",
    "* **`n_iter`**: This parameter specifies the number of iterations to perform during the randomized search. Each iteration samples a set of hyperparameters from the specified param_distributions and fits the model using those hyperparameters. The higher the value of n_iter, the more exhaustive the search for the optimal hyperparameters will be.\n",
    "\n",
    "* **`random_state`**: This parameter controls the random number generator used for the randomized search. Setting a specific value for random_state ensures that the same set of hyperparameters is sampled on each run, making the results reproducible. If random_state is not set, the search will generate different hyperparameters each time it is run.\n",
    "\n",
    "---\n",
    "\n",
    "**`RandomizedSearchCV`** has the same attributes as **`GridSearchCV`**.\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "params = {...}\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=model, \n",
    "                                   param_distribution=params,\n",
    "                                   n_iter=...)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "results = random_search.cv_results_\n",
    "```\n",
    "\n",
    "**<font color='blue'>1. Use the following param_distribution in the RandomizedSeachCV class to perform a first coarse search with `n_iter=500`.<br>How many trials would it be if used on along with a Grid Search strategy ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtfQIkSE4HvZ"
   },
   "outputs": [],
   "source": [
    "max_depth = list(range(5,80,5))\n",
    "max_depth.append(None)\n",
    "\n",
    "params = {\"model__criterion\": [\"gini\", \"entropy\"], \n",
    "          \"model__splitter\": [\"random\", \"best\"],\n",
    "          \"model__max_depth\": max_depth, \n",
    "          \"model__min_samples_split\": range(2,30,2), \n",
    "          \"model__min_samples_leaf\": range(1,30,2), \n",
    "          \"model__max_features\": range(1,7)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LyPX-O5yazv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPW6Qutahwe9"
   },
   "source": [
    "**<font color='blue'>2.a. Get the result of all the trials using the .cv_results_ attribute and sort the trials by the `rank_test_score` column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txBOrhejybTZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb_X9qtsh4Kj"
   },
   "source": [
    "**<font color='blue'>5.b. Have a look to the top hyperparameter combinations and especially their values.<br> Can you find similar values in different top combinations ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1680201171794,
     "user": {
      "displayName": "Antoine Palisson",
      "userId": "06334333839821267204"
     },
     "user_tz": -120
    },
    "id": "6Ky46Deqybyg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L8rIXTjkLjs"
   },
   "source": [
    "**<font color='blue'>3. Using the knowledge of the first 500 trials, reduce the spaces close the best values for each parameter and run 250 trials.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQpbILrTycVh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqILkMm1kwaX"
   },
   "source": [
    "**<font color='blue'>4. Get the result of all the trials and have a look to the top hyperparameter combinations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T70Vfi12yemQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DylSfk0MxQW2"
   },
   "source": [
    "**<font color='blue'>5.a. Get the best hyperparameter combination and re-train the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3BH71B8yfLx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hm9Rs4qaxZW9"
   },
   "source": [
    "**<font color='blue'>5.b. Predict the training, validation (using a cross-validation) and testing sets and get the scores.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDEQVvj4ygKq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOLbfyDupAa08QjMq3CJjYW",
   "collapsed_sections": [
    "RdaKgT1-HeQT",
    "EdtV_k87HhlQ",
    "qGywz0MqJLqt",
    "uwh_c2dBRxOV",
    "jYzN-ZjuR1kI",
    "m1gGR34BdxZJ"
   ],
   "provenance": [
    {
     "file_id": "1mU8AZURstgvbGkKWqz28G7EKsk-SYiFn",
     "timestamp": 1679907264873
    },
    {
     "file_id": "1SfyXGoHgEmZeAAKDDSZX-nox-BU3UClI",
     "timestamp": 1677191951629
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
